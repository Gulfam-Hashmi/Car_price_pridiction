{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a834a48",
   "metadata": {},
   "source": [
    "# Phase 1: Data Loading and String-to-Numeric Sanitization\n",
    "\n",
    "In this initial phase, we address the primary challenge of the dataset: **Data Types**. In the raw CSV, numerical features such as `HorsePower`, `Price`, and `Total Speed` are stored as string objects because they include units (e.g., 'hp', 'km/h') and currency symbols.\n",
    "\n",
    "### Methodology:\n",
    "To prepare the data for mathematical analysis and machine learning, we implemented a robust cleaning pipeline:\n",
    "\n",
    "1. **Unit Stripping:** We use a custom function to remove commas and specific units (case-insensitive).\n",
    "2. **Range Resolution:** A significant portion of the data contains ranges (e.g., `$12,000 - $15,000` or `70-85 hp`). Our logic splits these strings, converts the individual values to floats, and calculates the **arithmetic mean** to provide a single representative data point.\n",
    "3. **Type Conversion:** All cleaned strings are cast to 64-bit floats using NumPy logic.\n",
    "\n",
    "\n",
    "\n",
    "### Summary Statistics:\n",
    "The final step of this block generates **Descriptive Statistics** (Mean, Median, Std Dev, Min/Max) for the cleaned columns. This allows us to verify the success of the conversion and identify initial data trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset with the correct encoding\n",
    "df = pd.read_csv('Cars Datasets 2025.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "# Function to clean string columns (handles units, commas, and ranges)\n",
    "def clean_range_column(series, units_to_remove):\n",
    "    # 1. Remove non-numeric characters (units, commas)\n",
    "    cleaned = series.astype(str).str.replace(',', '', regex=False).str.replace(units_to_remove, '', regex=False, case=False).str.strip()\n",
    "\n",
    "    # 2. Function to handle ranges (e.g., '70-85' becomes 77.5)\n",
    "    def handle_range(value):\n",
    "        if value.lower() in ('nan', 'n/a', ''):\n",
    "            return np.nan\n",
    "        value = value.strip('$').strip()\n",
    "\n",
    "        if '-' in value:\n",
    "            try:\n",
    "                parts = [float(p.strip()) for p in value.split('-') if p.strip()]\n",
    "                return np.mean(parts) if len(parts) >= 2 else (parts[0] if parts else np.nan)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        else:\n",
    "            try:\n",
    "                return float(value)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "\n",
    "    return cleaned.apply(handle_range)\n",
    "\n",
    "# Apply cleaning to the required columns\n",
    "df['HorsePower_Clean'] = clean_range_column(df['HorsePower'], 'hp')\n",
    "df['Total_Speed_Clean'] = clean_range_column(df['Total Speed'], 'km/h')\n",
    "df['Performance_Clean'] = clean_range_column(df['Performance(0 - 100 )KM/H'], 'sec')\n",
    "df['Cars Prices_Clean'] = clean_range_column(df['Cars Prices'], '')\n",
    "df['Seats_Clean'] = clean_range_column(df['Seats'], '')\n",
    "df['Torque_Clean'] = clean_range_column(df['Torque'], 'Nm')\n",
    "\n",
    "print(\"--- Initial Cleaning & Summary Statistics Complete ---\")\n",
    "print(df[['HorsePower_Clean', 'Cars Prices_Clean']].describe().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Save the initially cleaned data\n",
    "df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7b6de",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA) & Data Visualization\n",
    "\n",
    "With the dataset sanitized into numerical values, we now perform a deep dive into the data to identify patterns and statistical relationships. This phase fulfills the requirement for 10-15 different analyses through visualizations and grouped aggregations.\n",
    "\n",
    "### Key Analyses Performed:\n",
    "\n",
    "1. **Correlation Analysis:** We use a **Heatmap** to determine how features like `HorsePower` and `Total Speed` relate to the `Price`. This helps us understand which variables are the strongest predictors for our machine learning model.\n",
    "2. **Outlier Detection:** Using **Box Plots**, we identify extreme data points. The car market is unique because luxury hyper-cars (outliers) can skew the average price significantly.\n",
    "3. **Distribution Analysis:** We use **Histograms** with Kernel Density Estimates (KDE) to see if our data follows a normal distribution or if it is skewed toward specific values.\n",
    "4. **Categorical Standardization:** We unify the `Fuel Types` column (e.g., merging various \"Hybrid\" labels) to prepare for categorical encoding.\n",
    "5. **Grouped Aggregations:** we calculate the **Mean Horsepower by Fuel Type** to see how performance varies across different engine technologies.\n",
    "6. **Manufacturer Trends:** We visualize the **Top 10 Companies** to see which brands dominate the dataset.\n",
    "\n",
    "### Visual Outputs:\n",
    "* `correlation_heatmap.png`: Reveals the \"internal logic\" of car specs.\n",
    "* `outlier_boxplots.png`: Highlights the gap between standard and luxury vehicles.\n",
    "* `horsepower_distribution.png`: Shows the frequency of different power levels.\n",
    "* `top_10_companies.png`: Displays market representation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8614d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data from the previous step\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Define Categorical Standardization (CRUCIAL for later encoding)\n",
    "def standardize_fuel(fuel):\n",
    "    fuel = str(fuel).lower()\n",
    "    if 'petrol' in fuel and 'diesel' in fuel:\n",
    "        return 'Petrol/Diesel'\n",
    "    elif 'electric' in fuel or 'ev' in fuel:\n",
    "         return 'Electric'\n",
    "    elif 'hybrid' in fuel or 'hyrbrid' in fuel or 'plug-in' in fuel:\n",
    "        return 'Hybrid'\n",
    "    elif 'diesel' in fuel:\n",
    "        return 'Diesel'\n",
    "    elif 'cng' in fuel:\n",
    "        return 'CNG'\n",
    "    elif 'hydrogen' in fuel:\n",
    "        return 'Hydrogen'\n",
    "    elif 'petrol' in fuel:\n",
    "        return 'Petrol'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['Fuel_Type_Standard'] = df['Fuel Types'].apply(standardize_fuel)\n",
    "\n",
    "# Numerical columns for analysis\n",
    "cleaned_cols = ['HorsePower_Clean', 'Total_Speed_Clean', 'Performance_Clean', 'Cars Prices_Clean']\n",
    "\n",
    "# --- EDA Requirements ---\n",
    "\n",
    "# 1. Correlation Analysis (Visualization: correlation_heatmap.png)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[cleaned_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Outlier Detection (Visualization: outlier_boxplots.png)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df['HorsePower_Clean'])\n",
    "plt.title('Box Plot of HorsePower')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['Cars Prices_Clean'])\n",
    "plt.ylim(0, 500000) # Zoomed in\n",
    "plt.title('Box Plot of Car Prices (Zoomed)')\n",
    "plt.savefig('outlier_boxplots.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Feature Distribution (Visualization: horsepower_distribution.png)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['HorsePower_Clean'].dropna(), kde=True, bins=30)\n",
    "plt.title('Distribution of HorsePower')\n",
    "plt.savefig('horsepower_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Grouped Aggregation & Categorical Count (Visualization: top_10_companies.png)\n",
    "top_10_companies = df['Company Names'].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=top_10_companies.index, y=top_10_companies.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Top 10 Companies by Car Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_10_companies.png')\n",
    "plt.close()\n",
    "\n",
    "# Print text results\n",
    "print(\"\\n--- 5. Grouped Aggregation: Mean HorsePower by Fuel Type ---\")\n",
    "print(df.groupby('Fuel_Type_Standard')['HorsePower_Clean'].mean().sort_values(ascending=False).to_markdown())\n",
    "\n",
    "print(\"\\n--- 6. Data Types and Unique Value Counts ---\")\n",
    "print(df.dtypes.to_frame(name='Data Type').to_markdown())\n",
    "\n",
    "\n",
    "# Save the fully cleaned data for ML\n",
    "df.to_csv('cleaned_data_final.csv', index=False)\n",
    "print(\"\\nEDA complete. All 4 PNG files and 'cleaned_data_final.csv' saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882876e",
   "metadata": {},
   "source": [
    "# Phase 3: Manual Data Preprocessing & Feature Engineering\n",
    "\n",
    "To prepare our data for a Linear Regression model without relying on external machine learning libraries, we implement the core preprocessing steps manually. This ensures our model receives clean, normalized, and numerically consistent data.\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "1. **Missing Value Imputation:** We identify missing values in numerical columns and fill them with the **Median**. The median is used instead of the mean because it is more robust against the extreme outliers (hyper-cars) identified in Phase 2.\n",
    "2. **Standard Scaling (Z-score Normalization):** Linear Regression is sensitive to the scale of data. If `Horsepower` ranges from 50 to 1000 and `Seats` ranges from 2 to 7, the model might incorrectly prioritize the larger numbers. We transform every numerical feature so it has a **mean of 0 and a standard deviation of 1** using the formula:  \n",
    "   $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "3. **One-Hot Encoding:** Machines cannot understand \"Petrol\" or \"Diesel\" as text. We convert the `Fuel Type` category into multiple binary columns (0 or 1). This is a pure Pandas implementation of categorical encoding.\n",
    "4. **Manual Train/Test Split:** To evaluate our model fairly, we shuffle the data using a random seed and split it:\n",
    "   * **80% Training Set:** Used to teach the model.\n",
    "   * **20% Testing Set:** Used to verify the model's accuracy on unseen data.\n",
    "\n",
    "### Configuration Export:\n",
    "We save the means, standard deviations, and median values into `preprocessing_config.json`. This is a critical step that allows our **Streamlit application** to apply the exact same mathematical transformations to user inputs at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51211c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random # Used for simple train/test split\n",
    "\n",
    "# Load the final data\n",
    "df = pd.read_csv('cleaned_data_final.csv')\n",
    "\n",
    "# Drop rows with missing target (price)\n",
    "df_clean = df.dropna(subset=['Cars Prices_Clean']).copy()\n",
    "\n",
    "# Features (X) and Target (Y)\n",
    "X = df_clean[['HorsePower_Clean', 'Total_Speed_Clean', 'Performance_Clean', 'Seats_Clean', 'Fuel_Type_Standard']].copy()\n",
    "Y = df_clean['Cars Prices_Clean'].copy()\n",
    "numerical_features = ['HorsePower_Clean', 'Total_Speed_Clean', 'Performance_Clean', 'Seats_Clean']\n",
    "\n",
    "# --- 1. Handling Missing Values (Imputation with Median) ---\n",
    "imputation_stats = {}\n",
    "for col in numerical_features:\n",
    "    median_val = X[col].median()\n",
    "    X[col] = X[col].fillna(median_val)\n",
    "    imputation_stats[col] = median_val # Save median for Streamlit\n",
    "\n",
    "print(\"1. Missing values imputed with the median.\")\n",
    "\n",
    "# --- 2. Scaling Numerical Features (Standard Scaling) ---\n",
    "scaling_stats = {}\n",
    "for col in numerical_features:\n",
    "    mean_val = X[col].mean()\n",
    "    std_val = X[col].std()\n",
    "    \n",
    "    # Standard Scaling Formula: (x - mean) / std\n",
    "    X[col] = (X[col] - mean_val) / std_val\n",
    "    \n",
    "    scaling_stats[col] = {'mean': mean_val, 'std': std_val} # Save stats for Streamlit\n",
    "\n",
    "print(\"2. Numerical features scaled (Z-score).\")\n",
    "\n",
    "# --- 3. Encoding Categorical Features (One-Hot Encoding with Pandas) ---\n",
    "X_encoded = pd.get_dummies(X, columns=['Fuel_Type_Standard'], drop_first=True, dtype=int)\n",
    "X_encoded = X_encoded.drop(columns=['Fuel_Type_Standard'], errors='ignore') \n",
    "\n",
    "final_columns = X_encoded.columns.tolist()\n",
    "\n",
    "# --- 4. Custom Train/Test Split (Pure Python/NumPy logic) ---\n",
    "# Create a list of indices, shuffle, and split\n",
    "indices = list(range(len(X_encoded)))\n",
    "random.seed(42) # Set seed for reproducibility\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * len(X_encoded))\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "X_train = X_encoded.iloc[train_indices]\n",
    "X_test = X_encoded.iloc[test_indices]\n",
    "Y_train = Y.iloc[train_indices]\n",
    "Y_test = Y.iloc[test_indices]\n",
    "\n",
    "\n",
    "# --- 5. Save Preprocessing Stats (CUSTOM JSON) ---\n",
    "preprocessing_config = {\n",
    "    'imputation_stats': imputation_stats,\n",
    "    'scaling_stats': scaling_stats,\n",
    "    'final_columns': final_columns,\n",
    "    # Convert DataFrame columns to list for saving categorical levels\n",
    "    'categorical_levels': X_encoded.filter(regex='Fuel_Type_Standard_').columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessing_config.json', 'w') as f:\n",
    "    json.dump(preprocessing_config, f)\n",
    "    \n",
    "print(\"\\nPreprocessing complete. Config saved as 'preprocessing_config.json'.\")\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e3153",
   "metadata": {},
   "source": [
    "# Phase 4: Machine Learning - Manual Linear Regression Implementation\n",
    "\n",
    "In the final phase of development, we build and train our predictive model. Rather than using \"black-box\" libraries, we implement **Linear Regression** using pure **NumPy** to solve the **Normal Equation**. This mathematical approach finds the specific weights that minimize the squared error between our predictions and the actual car prices.\n",
    "\n",
    "### Technical Implementation:\n",
    "\n",
    "1. **The Normal Equation:** We use matrix algebra to solve for the optimal parameter vector $\\theta$ (weights and bias) using the formula:\n",
    "   $$\\theta = (X^T X)^{-1} X^T y$$\n",
    "   * This involves transposing matrices, performing matrix multiplication (`@`), and calculating the inverse of the matrix.\n",
    "2. **Prediction Logic:** Once trained, the model predicts prices using the linear combination formula:\n",
    "   $$Y_{predicted} = X \\cdot weights + bias$$\n",
    "3. **Manual Performance Metrics:** To evaluate the model's accuracy on the unseen test set, we manually calculate:\n",
    "   * **Root Mean Squared Error (RMSE):** Represents the standard deviation of the prediction errors.\n",
    "   * **R-squared ($R^2$):** Indicates the proportion of the variance for the car price that is explained by our features (Horsepower, Speed, etc.).\n",
    "\n",
    "### Model Persistence:\n",
    "The final weights ($W$) and bias ($b$), along with the evaluation metrics, are exported to `linear_model_weights.json`. This \"lightweight\" model file is what powers the **runtime predictions** in our Streamlit application, allowing users to get instant price estimates without needing to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00005c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse X_train, Y_train, X_test, Y_test from the previous block (Code Block 3)\n",
    "\n",
    "# --- Linear Regression Class using Pure NumPy (Normal Equation) ---\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1. Add bias term (column of ones) to X\n",
    "        X_with_bias = X.copy()\n",
    "        X_with_bias.insert(0, 'Bias', 1.0)\n",
    "        \n",
    "        X_np = X_with_bias.values\n",
    "        y_np = y.values.reshape(-1, 1)\n",
    "\n",
    "        # 2. Normal Equation: W = (X^T * X)^-1 * X^T * y\n",
    "        # We use the @ operator for matrix multiplication\n",
    "        # np.linalg.inv calculates the inverse of a matrix\n",
    "        try:\n",
    "            W_np = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n",
    "            \n",
    "            self.bias = W_np[0, 0]\n",
    "            self.weights = W_np[1:, 0]\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Warning: Matrix is singular. Cannot compute inverse. Weights set to zero.\")\n",
    "            self.bias = 0.0\n",
    "            self.weights = np.zeros(X_np.shape[1] - 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Prediction: Y = XW + b\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_np = X.values\n",
    "        else:\n",
    "            X_np = X \n",
    "            \n",
    "        return X_np @ self.weights + self.bias\n",
    "        \n",
    "# --- Train the Model ---\n",
    "lr_model = SimpleLinearRegression()\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# --- Evaluate the Model (Manual Metrics) ---\n",
    "Y_pred_test = lr_model.predict(X_test)\n",
    "Y_test_np = Y_test.values\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(np.mean((Y_test_np - Y_pred_test)**2))\n",
    "\n",
    "# Calculate R2 Score\n",
    "SS_res = np.sum((Y_test_np - Y_pred_test)**2)\n",
    "SS_tot = np.sum((Y_test_np - Y_test_np.mean())**2)\n",
    "r2_score_val = 1 - (SS_res / SS_tot)\n",
    "\n",
    "print(\"\\n--- Linear Regression Model Results (Pure NumPy) ---\")\n",
    "print(f\"R-squared (R2): {r2_score_val:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")\n",
    "\n",
    "# --- Save the Model Weights (CUSTOM JSON) ---\n",
    "model_weights = {\n",
    "    'bias': lr_model.bias,\n",
    "    'weights': lr_model.weights.tolist(),\n",
    "    # Save the final test metrics for display in Streamlit\n",
    "    'test_r2': r2_score_val,\n",
    "    'test_rmse': rmse\n",
    "}\n",
    "\n",
    "with open('linear_model_weights.json', 'w') as f:\n",
    "    json.dump(model_weights, f)\n",
    "    \n",
    "print(\"Model weights saved as 'linear_model_weights.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
